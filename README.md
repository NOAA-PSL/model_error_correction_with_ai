# 0)set up directory where training data will be stored.

```
mkdir -p $WORKDIR/my_network/checks
cd $WORKDIR/my_network/

#clone repo 
mkdir code
cd code
git clone https://github.com/NOAA-PSL/model_error_correction_with_ai.git ./
```

checkout specific version of the code (here assume branch feature/stefan_10years_newjd)
```
git fetch origin feature/stefan_10years_newjd
git checkout feature/stefan_10years_newjd
```
add this code directory to the python path 
`export PYTHONPATH="${PYTHONPATH}:${PWD}"`

# 1) downscale original netcdf files to target resolution to reduce the data volume. This can be done with cdo as:
see for example reduce_dataset.sh  
note that we no longer need spectral resolution reduction files in reduce_dataset.sh produced with the "-gp2sp" option  
I found that 8-degree resolution works as well as 3 degree resolution. so i suggest using 8 degree resolution in the future.  

example of subsampled files on hera is in /scratch2/BMC/gsienkf/Sergey.Frolov/fromStefan/SUB8/  

# 2) preprocess a collection of netcdf files into a single numpy file. This numpy file has only the varaibles used for the network training. If you need to change inputs to the network, you will need to modify preprocess.py accordingly. 

## 2.a) modify  preprocess.py to point to correct location of files
line 15: dataDir -- location of netcdf files  
line 16: npyDir -- location of the output files  
line 26: dates -- range of dates for training  

## 2.b) modify preprocess.py to include the input variables as needed  

## 2.c) run preprocess.py to generate non-normalized dataset  
set line 13: `NORMALIZE=False`  
run preprocess.py in the batch queue usingg a single node.   
`Number of parallel threads are set on line 124: Parallel(n_jobs=40,verbose=0)(`

## 2.d) compute normalization coefficints  
modify get_mean_std.py to point to the output of 2.c  
run get_mean_std.py  

## 2.e) re-run preprocess.py with pre-computed mean and std  
set line 13: `NORMALIZE=True`  
run preprocess.py in the batch queue usingg a single node.  

# 3) training
## 3.a) modify varaible ddd in check_model.py, training.py to point to correct location of numpy data that was genrated by preprocess.py  

## 3.b) set up directory where training data will be stored.  
```
mkdir -p ~/work/my_network/checks
cd ~/work/my_network/
```

## 3.c) change to the working directory. e.g.  `$WORKDIR/my_network/`  

3.b) modify batch_training_parallel_GPU.py  
specifically, in the list below you might want to modify the length of the training period  
`ptmp=[device, 't', 4, '1', '4096', 3, 0.25, 32, 'mse', 0.0001, 1., 366,  365, 0.7]` 

e.g. 366 above refers to the end of the training period in days and 365 refers to the begging of the trainign period in days.  
the training period is in reference to the data record stored in the numpy files generated by preprocess.py and spcified by varible "dates"  

## 3.e) run training using my_job_gpu.sh  
you will need to set `$RUNDIR=$WORKDIR/my_network/` using example run directory we used before  

# 4) convert trained torch files to netcdf files used by UFS model  
modify and run torch_to_netcdf.py using the following python executable `/scratch1/NCEPDEV/global/Tse-chun.Chen/anaconda3/envs/ltn/bin/python`  

# 5) verify prediction for a single time using predict_bias_netcdf.py


 

